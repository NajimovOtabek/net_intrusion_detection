{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/otabek5454/net_intrusion_detection/blob/master/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9RGx3hvUJvt",
        "outputId": "b12026a1-f2ee-479f-9b1b-6b4344c623b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-t3RdDpmqMs4ABt9oobSapeNYTZJ9tpu\n",
            "To: /content/MachineLearningCSV.zip\n",
            "100% 235M/235M [00:05<00:00, 43.0MB/s]\n",
            "Archive:  MachineLearningCSV.zip\n",
            "   creating: MachineLearningCVE/\n",
            "  inflating: MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv  \n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1-t3RdDpmqMs4ABt9oobSapeNYTZJ9tpu\n",
        "!unzip MachineLearningCSV.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeAGjQlZx6ro",
        "outputId": "235172ff-c5f4-41ec-c219-eafc5dfc9361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-30 00:06:55--  https://raw.githubusercontent.com/Jumabek/net_intrusion_detection/develop/models.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15128 (15K) [text/plain]\n",
            "Saving to: ‘models.py’\n",
            "\n",
            "models.py           100%[===================>]  14.77K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-30 00:06:56 (138 MB/s) - ‘models.py’ saved [15128/15128]\n",
            "\n",
            "--2023-07-30 00:06:56--  https://raw.githubusercontent.com/Jumabek/net_intrusion_detection/develop/preprocessing.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3790 (3.7K) [text/plain]\n",
            "Saving to: ‘preprocessing.py’\n",
            "\n",
            "preprocessing.py    100%[===================>]   3.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-30 00:06:56 (52.9 MB/s) - ‘preprocessing.py’ saved [3790/3790]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/Jumabek/net_intrusion_detection/develop/models.py\n",
        "!wget https://raw.githubusercontent.com/Jumabek/net_intrusion_detection/develop/preprocessing.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Jc4d957fw2jH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from preprocessing import load_data, balance_data, normalize\n",
        "from models import Classifier\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from os.path import join\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKOdfcw9k5ie",
        "outputId": "93f78639-5632-4746-fdbf-a6fa2a645923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[########################################] | 100% Completed | 30.21 s\n",
            "[########################################] | 100% Completed | 29.56 s\n",
            "there are 2830743 flow records with 79 feature dimension\n",
            "stripped column names\n",
            "dropped bad columns\n",
            "There are 0 nan entries\n",
            "converted to numeric\n"
          ]
        }
      ],
      "source": [
        "dataroot = 'MachineLearningCVE/'\n",
        "\n",
        "from preprocessing import read_data\n",
        "data = read_data(dataroot,'*.pcap_ISCX.csv')\n",
        "\n",
        "# Load and preprocess the data\n",
        "X, y = load_data(dataroot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V7Yug4oANbsn"
      },
      "outputs": [],
      "source": [
        "X, y = balance_data(X, y, seed=42)\n",
        "X = normalize(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-4gGvttsxAoI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and remaining sets first\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "\n",
        "# Split the remaining data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)\n",
        "\n",
        "# Convert the data into PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7GRaCT-1xGet"
      },
      "outputs": [],
      "source": [
        "# Create PyTorch data loaders\n",
        "batch_size = 32\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "val_data = TensorDataset(X_val, y_val)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
        "\n",
        "device = \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6RzKiQtkxHRX"
      },
      "outputs": [],
      "source": [
        "#hyper-parameters\n",
        "hyper_params = {\n",
        "    'batch_size': 5120,\n",
        "    'optim': 'RMSProp',\n",
        "    'learning_rates': [1e-4, 1e-3],\n",
        "    'regularizations': [1e-4, 1e-3],\n",
        "    'num_layers': 3,\n",
        "    'method': 'cnn5',\n",
        "    'num_epochs': 10\n",
        "}\n",
        "\n",
        "# Determine input dimensions and number of classes\n",
        "input_dim = X_train.shape[1]\n",
        "num_class = len(np.unique(y_train))\n",
        "\n",
        "# Initialize tracking variables\n",
        "best_model, best_acc = None, -1\n",
        "accuracies = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jhhRMkCjIViR"
      },
      "outputs": [],
      "source": [
        "def getClassifier(args, runs_dir=None):\n",
        "\n",
        "    (method, optim, lr, reg, batch_size, input_dim, num_class, num_epochs) = args\n",
        "    if runs_dir is not None:\n",
        "        ensure_dir(runs_dir)\n",
        "\n",
        "    clf = Classifier(method, input_dim, num_class, lr=lr, reg=reg, num_epochs=num_epochs,\n",
        "                        batch_size=batch_size, runs_dir=runs_dir)\n",
        "\n",
        "    return clf\n",
        "\n",
        "    return clf\n",
        "def ensure_dir(dir_path):\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BpOeJZxcI6Vc"
      },
      "outputs": [],
      "source": [
        "# Setting up device\n",
        "device = \"cuda\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XO1i6UavI-ob"
      },
      "outputs": [],
      "source": [
        "# Convert data types\n",
        "X_train = X_train.float()\n",
        "y_train = y_train.long()\n",
        "\n",
        "# Move data to the device\n",
        "X_train = X_train.to(device)\n",
        "y_train = y_train.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data into numpy arrays\n",
        "X_train, y_train = X_train.cpu().numpy(), y_train.cpu().numpy()\n",
        "X_val, y_val = X_val.cpu().numpy(), y_val.cpu().numpy()\n",
        "X_test, y_test = X_test.cpu().numpy(), y_test.cpu().numpy()\n"
      ],
      "metadata": {
        "id": "NfI20q7vEheY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid search over learning rates and regularization strengths\n",
        "for lr in hyper_params['learning_rates']:\n",
        "    for reg in hyper_params['regularizations']:\n",
        "        # Create model configuration\n",
        "        config =  f\"{hyper_params['method']}/train/optim_{hyper_params['num_layers']}_{hyper_params['optim']}_lr_{lr}_reg_{reg}_bs_{hyper_params['batch_size']}\"\n",
        "        runs_dir = join(dataroot, 'runs', config)\n",
        "\n",
        "        # Initialize classifier with current parameters\n",
        "        classifier_args = (hyper_params['method'], hyper_params['optim'], lr, reg, hyper_params['batch_size'], input_dim, num_class,10)\n",
        "        clf = getClassifier(classifier_args, runs_dir)\n",
        "\n",
        "        # Train and evaluate model\n",
        "        start_time = time.time()\n",
        "        clf.fit(X_train, y_train)\n",
        "        predictions = clf.predict(X_test)\n",
        "        acc = metrics.balanced_accuracy_score(y_test, predictions)\n",
        "\n",
        "        # Update best model if current model is better\n",
        "        if acc > best_acc:\n",
        "            best_model = clf\n",
        "            best_acc = acc\n",
        "\n",
        "        # Store accuracy for current hyper-parameters\n",
        "        accuracies[(lr, reg)] = acc\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"Model trained in {end_time - start_time:.0f} sec\")\n",
        "\n",
        "# Display accuracies\n",
        "print(\"Accuracies for different hyper-parameters:\")\n",
        "for params, acc in accuracies.items():\n",
        "    print(f\"LR: {params[0]}, Reg: {params[1]}, Acc: {acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQSIhSQlHp5T",
        "outputId": "558126da-0f04-44d6-9577-e6fc5c256eb4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best epoch 0, best batch 0\n",
            "bst acc  -1\n",
            "Epoch [1/10], Step [50/348], Loss: 0.4381\n",
            "Epoch [1/10], Step [100/348], Loss: 0.3292\n",
            "Epoch [1/10], Step [150/348], Loss: 0.2710\n",
            "Epoch [1/10], Step [200/348], Loss: 0.2455\n",
            "Epoch [1/10], Step [250/348], Loss: 0.2338\n",
            "Epoch [1/10], Step [300/348], Loss: 0.2109\n",
            "Epoch [2/10], Step [1/348], Loss: 0.2152\n",
            "Epoch [2/10], Step [51/348], Loss: 0.2188\n",
            "Epoch [2/10], Step [101/348], Loss: 0.2159\n",
            "Epoch [2/10], Step [151/348], Loss: 0.2132\n",
            "Epoch [2/10], Step [201/348], Loss: 0.2042\n",
            "Epoch [2/10], Step [251/348], Loss: 0.2153\n",
            "Epoch [2/10], Step [301/348], Loss: 0.2039\n",
            "Epoch [3/10], Step [2/348], Loss: 0.1790\n",
            "Epoch [3/10], Step [52/348], Loss: 0.1918\n",
            "Epoch [3/10], Step [102/348], Loss: 0.1966\n",
            "Epoch [3/10], Step [152/348], Loss: 0.1877\n",
            "Epoch [3/10], Step [202/348], Loss: 0.2081\n",
            "Epoch [3/10], Step [252/348], Loss: 0.1694\n",
            "Epoch [3/10], Step [302/348], Loss: 0.1918\n",
            "Epoch [4/10], Step [3/348], Loss: 0.1793\n",
            "Epoch [4/10], Step [53/348], Loss: 0.1732\n",
            "Epoch [4/10], Step [103/348], Loss: 0.1908\n",
            "Epoch [4/10], Step [153/348], Loss: 0.1814\n",
            "Epoch [4/10], Step [203/348], Loss: 0.1725\n",
            "Epoch [4/10], Step [253/348], Loss: 0.1763\n",
            "Epoch [4/10], Step [303/348], Loss: 0.1527\n",
            "Epoch [5/10], Step [4/348], Loss: 0.1777\n",
            "Epoch [5/10], Step [54/348], Loss: 0.1771\n",
            "Epoch [5/10], Step [104/348], Loss: 0.1738\n",
            "Epoch [5/10], Step [154/348], Loss: 0.1612\n",
            "Epoch [5/10], Step [204/348], Loss: 0.1561\n",
            "Epoch [5/10], Step [254/348], Loss: 0.1808\n",
            "Epoch [5/10], Step [304/348], Loss: 0.1501\n",
            "Epoch [6/10], Step [5/348], Loss: 0.1583\n",
            "Epoch [6/10], Step [55/348], Loss: 0.1612\n",
            "Epoch [6/10], Step [105/348], Loss: 0.1533\n",
            "Epoch [6/10], Step [155/348], Loss: 0.1557\n",
            "Epoch [6/10], Step [205/348], Loss: 0.1623\n",
            "Epoch [6/10], Step [255/348], Loss: 0.1486\n",
            "Epoch [6/10], Step [305/348], Loss: 0.1523\n",
            "Epoch [7/10], Step [6/348], Loss: 0.1478\n",
            "Epoch [7/10], Step [56/348], Loss: 0.1521\n",
            "Epoch [7/10], Step [106/348], Loss: 0.1390\n",
            "Epoch [7/10], Step [156/348], Loss: 0.1324\n",
            "Epoch [7/10], Step [206/348], Loss: 0.1357\n",
            "Epoch [7/10], Step [256/348], Loss: 0.1351\n",
            "Epoch [7/10], Step [306/348], Loss: 0.1389\n",
            "Epoch [8/10], Step [7/348], Loss: 0.1397\n",
            "Epoch [8/10], Step [57/348], Loss: 0.1419\n",
            "Epoch [8/10], Step [107/348], Loss: 0.1350\n",
            "Epoch [8/10], Step [157/348], Loss: 0.1473\n",
            "Epoch [8/10], Step [207/348], Loss: 0.1418\n",
            "Epoch [8/10], Step [257/348], Loss: 0.1315\n",
            "Epoch [8/10], Step [307/348], Loss: 0.1313\n",
            "Epoch [9/10], Step [8/348], Loss: 0.1273\n",
            "Epoch [9/10], Step [58/348], Loss: 0.1273\n",
            "Epoch [9/10], Step [108/348], Loss: 0.1236\n",
            "Epoch [9/10], Step [158/348], Loss: 0.1300\n",
            "Epoch [9/10], Step [208/348], Loss: 0.1294\n",
            "Epoch [9/10], Step [258/348], Loss: 0.1380\n",
            "Epoch [9/10], Step [308/348], Loss: 0.1244\n",
            "Epoch [10/10], Step [9/348], Loss: 0.1191\n",
            "Epoch [10/10], Step [59/348], Loss: 0.1752\n",
            "Epoch [10/10], Step [109/348], Loss: 0.1176\n",
            "Epoch [10/10], Step [159/348], Loss: 0.1267\n",
            "Epoch [10/10], Step [209/348], Loss: 0.1236\n",
            "Epoch [10/10], Step [259/348], Loss: 0.1580\n",
            "Epoch [10/10], Step [309/348], Loss: 0.1570\n",
            "Model trained in 1836 sec\n",
            "best epoch 0, best batch 0\n",
            "bst acc  -1\n",
            "Epoch [1/10], Step [50/348], Loss: 0.4508\n",
            "Epoch [1/10], Step [100/348], Loss: 0.3309\n",
            "Epoch [1/10], Step [150/348], Loss: 0.2748\n",
            "Epoch [1/10], Step [200/348], Loss: 0.2509\n",
            "Epoch [1/10], Step [250/348], Loss: 0.2369\n",
            "Epoch [1/10], Step [300/348], Loss: 0.2160\n",
            "Epoch [2/10], Step [1/348], Loss: 0.2153\n",
            "Epoch [2/10], Step [51/348], Loss: 0.2201\n",
            "Epoch [2/10], Step [101/348], Loss: 0.2204\n",
            "Epoch [2/10], Step [151/348], Loss: 0.2153\n",
            "Epoch [2/10], Step [201/348], Loss: 0.2087\n",
            "Epoch [2/10], Step [251/348], Loss: 0.2080\n",
            "Epoch [2/10], Step [301/348], Loss: 0.2036\n",
            "Epoch [3/10], Step [2/348], Loss: 0.1853\n",
            "Epoch [3/10], Step [52/348], Loss: 0.1942\n",
            "Epoch [3/10], Step [102/348], Loss: 0.1935\n",
            "Epoch [3/10], Step [152/348], Loss: 0.1918\n",
            "Epoch [3/10], Step [202/348], Loss: 0.2111\n",
            "Epoch [3/10], Step [252/348], Loss: 0.1689\n",
            "Epoch [3/10], Step [302/348], Loss: 0.1917\n",
            "Epoch [4/10], Step [3/348], Loss: 0.1811\n",
            "Epoch [4/10], Step [53/348], Loss: 0.1772\n",
            "Epoch [4/10], Step [103/348], Loss: 0.1974\n",
            "Epoch [4/10], Step [153/348], Loss: 0.1850\n",
            "Epoch [4/10], Step [203/348], Loss: 0.1754\n",
            "Epoch [4/10], Step [253/348], Loss: 0.1784\n",
            "Epoch [4/10], Step [303/348], Loss: 0.1602\n",
            "Epoch [5/10], Step [4/348], Loss: 0.1834\n",
            "Epoch [5/10], Step [54/348], Loss: 0.1841\n",
            "Epoch [5/10], Step [104/348], Loss: 0.1793\n",
            "Epoch [5/10], Step [154/348], Loss: 0.1678\n",
            "Epoch [5/10], Step [204/348], Loss: 0.1615\n",
            "Epoch [5/10], Step [254/348], Loss: 0.1677\n",
            "Epoch [5/10], Step [304/348], Loss: 0.1658\n",
            "Epoch [6/10], Step [5/348], Loss: 0.1638\n",
            "Epoch [6/10], Step [55/348], Loss: 0.1721\n",
            "Epoch [6/10], Step [105/348], Loss: 0.1593\n",
            "Epoch [6/10], Step [155/348], Loss: 0.1648\n",
            "Epoch [6/10], Step [205/348], Loss: 0.1625\n",
            "Epoch [6/10], Step [255/348], Loss: 0.1600\n",
            "Epoch [6/10], Step [305/348], Loss: 0.1652\n",
            "Epoch [7/10], Step [6/348], Loss: 0.1619\n",
            "Epoch [7/10], Step [56/348], Loss: 0.1626\n",
            "Epoch [7/10], Step [106/348], Loss: 0.1519\n",
            "Epoch [7/10], Step [156/348], Loss: 0.1425\n",
            "Epoch [7/10], Step [206/348], Loss: 0.1507\n",
            "Epoch [7/10], Step [256/348], Loss: 0.1548\n",
            "Epoch [7/10], Step [306/348], Loss: 0.1447\n",
            "Epoch [8/10], Step [7/348], Loss: 0.1571\n",
            "Epoch [8/10], Step [57/348], Loss: 0.1606\n",
            "Epoch [8/10], Step [107/348], Loss: 0.1530\n",
            "Epoch [8/10], Step [157/348], Loss: 0.1505\n",
            "Epoch [8/10], Step [207/348], Loss: 0.1544\n",
            "Epoch [8/10], Step [257/348], Loss: 0.1455\n",
            "Epoch [8/10], Step [307/348], Loss: 0.1526\n",
            "Epoch [9/10], Step [8/348], Loss: 0.1439\n",
            "Epoch [9/10], Step [58/348], Loss: 0.1395\n",
            "Epoch [9/10], Step [108/348], Loss: 0.1463\n",
            "Epoch [9/10], Step [158/348], Loss: 0.1427\n",
            "Epoch [9/10], Step [208/348], Loss: 0.1490\n",
            "Epoch [9/10], Step [258/348], Loss: 0.1367\n",
            "Epoch [9/10], Step [308/348], Loss: 0.1546\n",
            "Epoch [10/10], Step [9/348], Loss: 0.1378\n",
            "Epoch [10/10], Step [59/348], Loss: 0.1331\n",
            "Epoch [10/10], Step [109/348], Loss: 0.1301\n",
            "Epoch [10/10], Step [159/348], Loss: 0.1421\n",
            "Epoch [10/10], Step [209/348], Loss: 0.1359\n",
            "Epoch [10/10], Step [259/348], Loss: 0.1392\n",
            "Epoch [10/10], Step [309/348], Loss: 0.2118\n",
            "Model trained in 1835 sec\n",
            "best epoch 0, best batch 0\n",
            "bst acc  -1\n",
            "Epoch [1/10], Step [50/348], Loss: 0.2719\n",
            "Epoch [1/10], Step [100/348], Loss: 0.2894\n",
            "Epoch [1/10], Step [150/348], Loss: 0.2166\n",
            "Epoch [1/10], Step [200/348], Loss: 0.2185\n",
            "Epoch [1/10], Step [250/348], Loss: 0.2172\n",
            "Epoch [1/10], Step [300/348], Loss: 0.1749\n",
            "Epoch [2/10], Step [1/348], Loss: 0.1814\n",
            "Epoch [2/10], Step [51/348], Loss: 0.1909\n",
            "Epoch [2/10], Step [101/348], Loss: 0.2153\n",
            "Epoch [2/10], Step [151/348], Loss: 0.1776\n",
            "Epoch [2/10], Step [201/348], Loss: 0.1800\n",
            "Epoch [2/10], Step [251/348], Loss: 0.1750\n",
            "Epoch [2/10], Step [301/348], Loss: 0.2018\n",
            "Epoch [3/10], Step [2/348], Loss: 0.1689\n",
            "Epoch [3/10], Step [52/348], Loss: 0.1646\n",
            "Epoch [3/10], Step [102/348], Loss: 0.1636\n",
            "Epoch [3/10], Step [152/348], Loss: 0.1707\n",
            "Epoch [3/10], Step [202/348], Loss: 0.1691\n",
            "Epoch [3/10], Step [252/348], Loss: 0.1290\n",
            "Epoch [3/10], Step [302/348], Loss: 0.1547\n",
            "Epoch [4/10], Step [3/348], Loss: 0.1287\n",
            "Epoch [4/10], Step [53/348], Loss: 0.1266\n",
            "Epoch [4/10], Step [103/348], Loss: 0.1646\n",
            "Epoch [4/10], Step [153/348], Loss: 0.1505\n",
            "Epoch [4/10], Step [203/348], Loss: 0.1381\n",
            "Epoch [4/10], Step [253/348], Loss: 0.1567\n",
            "Epoch [4/10], Step [303/348], Loss: 0.1342\n",
            "Epoch [5/10], Step [4/348], Loss: 0.1331\n",
            "Epoch [5/10], Step [54/348], Loss: 0.1326\n",
            "Epoch [5/10], Step [104/348], Loss: 0.1838\n",
            "no improvement in accuracy for 10 iterations\n",
            "Model trained in 812 sec\n",
            "best epoch 0, best batch 0\n",
            "bst acc  -1\n",
            "Epoch [1/10], Step [50/348], Loss: 0.2875\n",
            "Epoch [1/10], Step [100/348], Loss: 0.2756\n",
            "Epoch [1/10], Step [150/348], Loss: 0.2252\n",
            "Epoch [1/10], Step [200/348], Loss: 0.2183\n",
            "Epoch [1/10], Step [250/348], Loss: 0.2170\n",
            "Epoch [1/10], Step [300/348], Loss: 0.1866\n",
            "Epoch [2/10], Step [1/348], Loss: 0.1856\n",
            "Epoch [2/10], Step [51/348], Loss: 0.2049\n",
            "Epoch [2/10], Step [101/348], Loss: 0.1843\n",
            "Epoch [2/10], Step [151/348], Loss: 0.1916\n",
            "Epoch [2/10], Step [201/348], Loss: 0.1886\n",
            "Epoch [2/10], Step [251/348], Loss: 0.1800\n",
            "Epoch [2/10], Step [301/348], Loss: 0.2028\n",
            "Epoch [3/10], Step [2/348], Loss: 0.1667\n",
            "Epoch [3/10], Step [52/348], Loss: 0.1870\n",
            "Epoch [3/10], Step [102/348], Loss: 0.1599\n",
            "Epoch [3/10], Step [152/348], Loss: 0.1830\n",
            "Epoch [3/10], Step [202/348], Loss: 0.2351\n",
            "Epoch [3/10], Step [252/348], Loss: 0.1425\n",
            "Epoch [3/10], Step [302/348], Loss: 0.1624\n",
            "Epoch [4/10], Step [3/348], Loss: 0.1704\n",
            "Epoch [4/10], Step [53/348], Loss: 0.1433\n",
            "Epoch [4/10], Step [103/348], Loss: 0.1484\n",
            "Epoch [4/10], Step [153/348], Loss: 0.1550\n",
            "Epoch [4/10], Step [203/348], Loss: 0.3969\n",
            "Epoch [4/10], Step [253/348], Loss: 0.1774\n",
            "Epoch [4/10], Step [303/348], Loss: 0.1544\n",
            "Epoch [5/10], Step [4/348], Loss: 0.1677\n",
            "Epoch [5/10], Step [54/348], Loss: 0.1641\n",
            "Epoch [5/10], Step [104/348], Loss: 0.1605\n",
            "Epoch [5/10], Step [154/348], Loss: 0.1512\n",
            "Epoch [5/10], Step [204/348], Loss: 0.1393\n",
            "no improvement in accuracy for 10 iterations\n",
            "Model trained in 873 sec\n",
            "Accuracies for different hyper-parameters:\n",
            "LR: 0.0001, Reg: 0.0001, Acc: 0.9457574657267204\n",
            "LR: 0.0001, Reg: 0.001, Acc: 0.9456271677862523\n",
            "LR: 0.001, Reg: 0.0001, Acc: 0.921253086129117\n",
            "LR: 0.001, Reg: 0.001, Acc: 0.9261437724426316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cq_G5eQtL7ir"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMmbQuzZhoCMS9TMoNQyZ3E",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}