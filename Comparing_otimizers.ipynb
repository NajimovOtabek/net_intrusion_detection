{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPNSdb8O74CYTJsKKuYa36q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NajimovOtabek/net_intrusion_detection/blob/master/Comparing_otimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1eT_1tt_A2t",
        "outputId": "08078fec-345e-4571-e3a0-01a93331a24b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-t3RdDpmqMs4ABt9oobSapeNYTZJ9tpu\n",
            "To: /content/MachineLearningCSV.zip\n",
            "100% 235M/235M [00:03<00:00, 74.6MB/s]\n",
            "Archive:  MachineLearningCSV.zip\n",
            "   creating: MachineLearningCVE/\n",
            "  inflating: MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv  \n",
            "  inflating: MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv  \n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1-t3RdDpmqMs4ABt9oobSapeNYTZJ9tpu\n",
        "!unzip MachineLearningCSV.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/Jumabek/net_intrusion_detection/develop/preprocessing.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVs3swvx_UN6",
        "outputId": "1cf813dd-077b-47a5-b0a7-8fb5137006cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-23 02:22:09--  https://raw.githubusercontent.com/Jumabek/net_intrusion_detection/develop/preprocessing.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3790 (3.7K) [text/plain]\n",
            "Saving to: ‘preprocessing.py’\n",
            "\n",
            "preprocessing.py    100%[===================>]   3.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-23 02:22:10 (45.6 MB/s) - ‘preprocessing.py’ saved [3790/3790]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score, classification_report\n",
        "from preprocessing import read_data, load_data, normalize\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "GhuMhVfs_WLb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataroot = 'MachineLearningCVE/'\n",
        "from preprocessing import read_data\n",
        "data = read_data(dataroot,'*.pcap_ISCX.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0Rav3K3_Z-c",
        "outputId": "d1072616-ec23-4c1e-d0fd-3884d74ad44e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[########################################] | 100% Completed | 28.24 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and normalize the data\n",
        "X, y = load_data(dataroot)\n",
        "X = normalize(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGQybLPl_iQ8",
        "outputId": "dde01864-8a21-4dd9-ac36-2a09e1a7239d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[########################################] | 100% Completed | 28.11 s\n",
            "there are 2830743 flow records with 79 feature dimension\n",
            "stripped column names\n",
            "dropped bad columns\n",
            "There are 0 nan entries\n",
            "converted to numeric\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stratified_split(X, y, test_size=0.1, random_state=42, min_class_count=2000):\n",
        "    # Identify classes with at least min_class_count instances\n",
        "    counts = Counter(y)\n",
        "    valid_classes = [cls for cls, count in counts.items() if count >= min_class_count]\n",
        "\n",
        "    # Filter the data to include only those classes\n",
        "    X_filtered = [x for x, cls in zip(X, y) if cls in valid_classes]\n",
        "    y_filtered = [cls for cls in y if cls in valid_classes]\n",
        "\n",
        "    # Stratified sampling to maintain the class distribution\n",
        "    split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
        "    for train_index, sample_index in split.split(X_filtered, y_filtered):\n",
        "        X_new = [X_filtered[i] for i in sample_index]\n",
        "        y_new = [y_filtered[i] for i in sample_index]\n",
        "\n",
        "    return X_new, y_new\n",
        "\n",
        "def filter_classes(X, y, threshold=100):\n",
        "    # Count the occurrences of each class in y\n",
        "    class_counts = Counter(y)\n",
        "\n",
        "    # Find the classes that meet the threshold\n",
        "    valid_classes = {cls for cls, count in class_counts.items() if count >= threshold}\n",
        "\n",
        "    # Filter the instances of X and y based on the valid classes\n",
        "    X_filtered = [x for x, cls in zip(X, y) if cls in valid_classes]\n",
        "    y_filtered = [cls for cls in y if cls in valid_classes]\n",
        "\n",
        "    return np.array(X_filtered), np.array(y_filtered)\n",
        "\n",
        "# Filter classes with fewer than 100 instances\n",
        "X, y = filter_classes(X, y)\n",
        "\n",
        "# Use only 10% of the data\n",
        "X, y = stratified_split(X, y)"
      ],
      "metadata": {
        "id": "3lFT3H4n_sbr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "def balance_data(X, y, target_samples=50000, seed=42):\n",
        "    # First, apply under-sampling to reduce the majority class\n",
        "    under_sampler = RandomUnderSampler(sampling_strategy={0: target_samples}, random_state=seed)\n",
        "    X_under, y_under = under_sampler.fit_resample(X, y)\n",
        "\n",
        "    # Next, apply SMOTE to increase the minority classes\n",
        "    smote = SMOTE(random_state=seed)\n",
        "    new_X, new_y = smote.fit_resample(X_under, y_under)\n",
        "\n",
        "    return new_X, new_y\n"
      ],
      "metadata": {
        "id": "tyfUsOMl_vlL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the Data\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)\n",
        "\n",
        "# Print class distribution before balancing\n",
        "unique_before, counts_before = np.unique(y, return_counts=True)\n",
        "print(\"Class distribution before balancing:\", dict(zip(unique_before, counts_before)))\n",
        "\n",
        "# Balance the data\n",
        "X_train, y_train = balance_data(X_train, y_train, target_samples=25000, seed=42)\n",
        "\n",
        "# Print class distribution after balancing\n",
        "unique_after, counts_after = np.unique(y_train, return_counts=True)\n",
        "print(\"Class distribution after balancing:\", dict(zip(unique_after, counts_after)))\n",
        "\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8q-ozWeCdb0",
        "outputId": "9c307e74-9b30-4b42-ebe3-23855e8a5051"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before balancing: {0: 227310, 2: 12803, 3: 1029, 4: 23107, 5: 550, 6: 579, 7: 794, 10: 15893, 11: 590}\n",
            "Class distribution after balancing: {0: 25000, 2: 25000, 3: 25000, 4: 25000, 5: 25000, 6: 25000, 7: 25000, 10: 25000, 11: 25000}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-fddd3811cad7>:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n"
      ],
      "metadata": {
        "id": "yM6Y9JCSDNPu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, d_model=64, nhead=4, num_layers=4, dim_feedforward=128, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, d_model)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.unsqueeze(1)  # Add a fake batch dimension for the transformer\n",
        "        x = self.transformer(x, x)  # Encoder-Decoder Self-Attention\n",
        "        x = x.squeeze(1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in data_loader:\n",
        "            data = data.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "    return accuracy, f1, precision, recall\n"
      ],
      "metadata": {
        "id": "s3BguSpo_1cp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
        "\n",
        "def evaluate_model(model, data_loader) -> tuple:\n",
        "    model.eval()\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in data_loader:\n",
        "            data = data.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    balanced_accuracy = balanced_accuracy_score(true_labels, predicted_labels)\n",
        "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "    macro_f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
        "    macro_precision = precision_score(true_labels, predicted_labels, average='macro')\n",
        "    macro_recall = recall_score(true_labels, predicted_labels, average='macro')\n",
        "\n",
        "    print(\"Classification Report : \\n\", classification_report(true_labels, predicted_labels))\n",
        "\n",
        "    return accuracy, balanced_accuracy, f1, precision, recall, macro_f1, macro_precision, macro_recall"
      ],
      "metadata": {
        "id": "hUMfnFu6Cmd8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam, SGD, Adagrad, RMSprop, AdamW\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def train_model(optimizer_class,y):\n",
        "    # Convert X_train to a NumPy array if it's a list\n",
        "    X_train_array = np.array(X_train)\n",
        "    input_dim = X_train_array.shape[1]\n",
        "    unique_labels = np.unique(y)\n",
        "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    y = np.vectorize(label_mapping.get)(y)\n",
        "    num_classes = len(unique_labels)\n",
        "\n",
        "    model = TransformerModel(input_dim, num_classes, num_layers=6)  # Adjust as needed\n",
        "    model = model.cpu()\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optimizer_class(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Dataloader with new batch size\n",
        "    batch_size = 1024\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, (data, labels) in enumerate(train_loader):\n",
        "            data = data.cpu()\n",
        "            labels = labels.cpu()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Optimizer: {optimizer.__class__.__name__}, Epoch [{epoch + 1}/10], Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "    # Evaluate model\n",
        "    # Ensure evaluate_model function is defined\n",
        "    acc, balanced_acc, f1, prec, rec, macro_f1, macro_prec, macro_rec = evaluate_model(model, DataLoader(test_dataset, batch_size=batch_size))\n",
        "    print(f'Optimizer: {optimizer.__class__.__name__}, Accuracy: {acc:.4f}, Balanced Accuracy: {balanced_acc:.4f}, F1: {f1:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, Macro F1: {macro_f1:.4f}, Macro Precision: {macro_prec:.4f}, Macro Recall: {macro_rec:.4f}')\n",
        "\n",
        "    return acc, balanced_acc, f1, prec, rec, macro_f1, macro_prec, macro_rec\n"
      ],
      "metadata": {
        "id": "EiXK7IF4_4nt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of optimizers\n",
        "optimizers_dict = {\n",
        "    'Adam': Adam,\n",
        "    'SGD': SGD,\n",
        "    'Adagrad': Adagrad,\n",
        "    'RMSprop': RMSprop,\n",
        "    'AdamW': AdamW\n",
        "}\n",
        "\n",
        "# Lists to store metrics for each model configuration\n",
        "accuracies = []\n",
        "balanced_accuracies = []\n",
        "f1_scores = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "macro_f1_scores = []\n",
        "macro_precisions = []\n",
        "macro_recalls = []\n",
        "\n",
        "for opt_name, opt in optimizers_dict.items():\n",
        "    acc, balanced_acc, f1, prec, rec, macro_f1, macro_prec, macro_rec = train_model(opt, y_train)\n",
        "    accuracies.append(acc)\n",
        "    balanced_accuracies.append(balanced_acc)\n",
        "    f1_scores.append(f1)\n",
        "    precisions.append(prec)\n",
        "    recalls.append(rec)\n",
        "    macro_f1_scores.append(macro_f1)\n",
        "    macro_precisions.append(macro_prec)\n",
        "    macro_recalls.append(macro_rec)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "9LZiQyFB_4qO",
        "outputId": "6b9efcbd-b75d-47d7-cdb3-6bc285fbea91"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-db46621eb8d0>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mopt_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizers_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbalanced_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmacro_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmacro_prec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmacro_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mbalanced_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbalanced_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-a2b1b83d9edd>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(optimizer_class, y)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 10 is out of bounds."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = np.unique(y)\n",
        "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "y = np.vectorize(label_mapping.get)(y)\n",
        "num_classes = len(unique_labels) # or 12 if the labels range from 0 to 11\n"
      ],
      "metadata": {
        "id": "omCX8Nn9HiuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of unique labels:\", len(np.unique(y)))\n",
        "print(\"Max label:\", np.max(y))\n",
        "print(\"Num classes:\", num_classes)\n",
        "assert np.max(y) < num_classes, \"Labels are out of range!\"\n",
        "\n"
      ],
      "metadata": {
        "id": "T014nADtHR-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create an index for each tick position\n",
        "x_indexes = np.arange(len(optimizers_dict.keys()))\n",
        "width = 0.2  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Generate bar graphs\n",
        "rects1 = ax.bar(x_indexes - 2 * width, accuracies, width, label='Accuracy')\n",
        "rects2 = ax.bar(x_indexes - width, f1_scores, width, label='F1 Score')\n",
        "rects3 = ax.bar(x_indexes, precisions, width, label='Precision')\n",
        "rects4 = ax.bar(x_indexes + width, recalls, width, label='Recall')\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_xlabel('Optimizers')\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Scores by Optimizers')\n",
        "ax.set_xticks(x_indexes)\n",
        "ax.set_xticklabels(optimizers_dict.keys())\n",
        "ax.legend()\n",
        "\n",
        "# Attach a text label above each bar in *rects*, displaying its height\n",
        "for rects in [rects1, rects2, rects3, rects4]:\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{:.2f}'.format(height),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nTp3ibUi_4wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"optimizers:\", optimizers_dict )\n",
        "print(\"accuracies:\", accuracies)\n",
        "print(\"f1_scores:\", f1_scores)\n",
        "print(\"precisions:\", precisions)\n",
        "print(\"recalls:\", recalls)"
      ],
      "metadata": {
        "id": "H9-Ikd4F_41Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ami9o-yT_44D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}